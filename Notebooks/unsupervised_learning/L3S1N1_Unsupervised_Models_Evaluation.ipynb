{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurements\n",
    "\n",
    "There are two things that we should have realised based on our first experiments with unsupervised training:\n",
    "\n",
    "a) It is not trivial to train and optimise machine learning models \n",
    "   - There are a lots of knobs and buttons to press and turn\n",
    "   - The choice of training material has a major impact but is very hard to control\n",
    "   \n",
    "b) It can be very time-intensive to evaluate a model.\n",
    "   - There might also be room for subjective bias if we evaluate \n",
    "   - If we train multiple times we would like to be able to conserve our evaluation efforts.\n",
    "   \n",
    "\n",
    "This notebook takes a look at two ways for evaluating machine learning models:\n",
    "\n",
    "* Quantitative Evaluation\n",
    "* Qualitative Evaluation\n",
    "\n",
    "It is important to note that the basis for all evaluation of machine learning models is always human judgement. \n",
    "In the end, right or wrong has to be based on some form of judgement from humans. \n",
    "This holds true for the quantitative as well as the qualitative evaluation.\n",
    "\n",
    "\n",
    "## Quantitative Evaluation\n",
    "\n",
    "`Quantitative Evaluation` is based on definining a metric and taking measurements relating to the models we train. This is in many cases referred to as the `accuracy` or `performance` of a trained model. \n",
    "Different metrics have different units and ranges (e.g. 0-1, 0-100%). \n",
    "\n",
    "### Baseline\n",
    "\n",
    "Different trained models are compared based on these measurements. \n",
    "It is usually custom to train a first model, evaluate it, and use those first measurements as a `baseline`.\n",
    "`Baseline` in this case just means that we create a solid first measurement which will serve as a comparison point for future model training.\n",
    "\n",
    "Quite often we also make use of `baselines` established by other people. E.g. baselines published in scientific papers. \n",
    "Those baselines are often used in order to make sure that our `set up` is correct. I.e. that our machine learning stack and the results that we achieve are in line with the `state of the art` (as good as we should expect them to be).\n",
    "\n",
    "### Validity\n",
    "\n",
    "Validity refers to the certainty that we measure what we want to measure.\n",
    "This may sound a bit strange at first \"measure what we want to measure\".\n",
    "\n",
    "Consider the following scenario.\n",
    "* You are tasked with creating an e-mail sorting system based on machine learning\n",
    "* The system is supposed to be used in production for all emails at a large multi-national company\n",
    "* In order to test the accuracy of your system you take 20 sample e-mails, have them sorted, and then count how many are correctly sorted.\n",
    "\n",
    "What are your thoughts towards these measurements?\n",
    "\n",
    "* Are we measuring the performance of our system?\n",
    "* How did we choose these 20 e-mails?\n",
    "* Does the method of choice influence what we measure? \n",
    "\n",
    "Once one starts to consider these aspects the question of validity quickly gains in complexity.\n",
    "Often people will then attempt to measure higher level concepts such as `user satisfaction`. \n",
    "However it turns out, that one cannot escape the question of validity by taking that route.\n",
    "On the contrary, the higher the abstraction level, the more complex the question of validity becomes.\n",
    "\n",
    "Thinking about validity is an extremely valuable asset in many engineering and business related aspects as measurements are the basis for all optimisations.\n",
    "\n",
    "\n",
    "## Qualitative Evaluation\n",
    "\n",
    "Qualitative evaluation is often somewhat neglected or not taken serious enough in professional set ups.\n",
    "Qualitative evaluation refers to all evaluation efforts that are not specifically based on measurement of 'hard' numbers (probably the reason why it is often not taken serious).\n",
    "\n",
    "Qualitivate evaluation can be useful for identifying a range of:\n",
    "\n",
    "* potential problems in the machine learning set up (noise in the data, bias in the training data, quality problems in the training data)\n",
    "* potential for optimisiations (e.g. additional pre-processing that might be beneficial, direct hyperparameters, introduction of new hyperparameters)\n",
    "\n",
    "Structured approaches to qualitative evaluation benefits from efficient ways to slice through the data represented by the model. This can consist of using visualisations, descriptive statistics, or smart sampling (choosing sub-sets) on basis of the trained models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing a Metric for Word2Vec\n",
    "\n",
    "\n",
    "Based on the last exercise we will try to implement a metric that helps us to measure how good a semantic space is. \n",
    "\n",
    "The metric is based on a word analogy task described in the following paper: https://arxiv.org/pdf/1607.04606v1.pdf.\n",
    "\n",
    "The semantic space we have trained is used to do predictions of the following form:\n",
    "\n",
    "    Example analogy: amazing amazingly calm calmly\n",
    "\n",
    "    This analogy is marked correct if:\n",
    "\n",
    "    embedding(amazing) – embedding(amazingly)\n",
    "\n",
    "    = embedding(calm) – embedding(calmly)\n",
    "    \n",
    "The model scores a point if it correctly predicts calmly given the first three elements. \n",
    "\n",
    "You can see the analogy pairs of the test set based on the following link:\n",
    "\n",
    "https://raw.githubusercontent.com/RaRe-Technologies/gensim/develop/gensim/test/test_data/questions-words.txt\n",
    "\n",
    "In order to use them for evaluation you will have to download the above file locally or to a remote notebook (depending on your setup).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a first step use the \"gensim.models.KeyedVectors.load_word2vec_format()\" method to load a model we have trained before. \n",
    "\n",
    "\n",
    "\n",
    "# run the evaluation based on calling \"model.wv.accuracy(\"./data/questions-words.txt\")\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results\n",
    "\n",
    "The results we will see will depend heavily on the dataset we use. \n",
    "\n",
    "This is a good example of domain specificity. The result of the training but also of the evaluation will depend heavily on the context from which the training material is taken. \n",
    "\n",
    "In general if we want to apply machine learning to very specified domains or business use cases we often have to create our own evaluation sets in order to make sure that our measurements are representative.\n",
    "\n",
    "If we use the pre-defined data set it is likely that we will see a lot of Out-Of-Vocabulary (OOV) events. I.e. the test set contains a lot of concepts that are not contained in our semantic space (because they were completely missing or did not appear often enough in our training data set for the machine to learn it).\n",
    "\n",
    "It should become clear how such quantitative evaluation can be useful for comparing models. \n",
    "However, it is important to always remember the question of validity and to consider effects such as OOV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Test Set Creation\n",
    "\n",
    "\n",
    "Based on the format and definition of the word analogy task create a small (up to 50 entries max) test set for the swiss text or review dataset that is more appropriate.\n",
    "\n",
    "For some background on the analogy task you can read up in the original publication under the \"Word Analogy Tasks\" section. \n",
    "\n",
    "Repeat the measurement with the custom test set you have created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation Based On T-SNE\n",
    "\n",
    "We will also take a look at qualitative evaluation based on another unsupervised learning technique called T-SNE.\n",
    "\n",
    "T-SNE takes as input elements that can be compared to each other and learns a representation that tries to optimise placing related items closer to each other (it is easy to imagine how this is a non-trivial task if we are dealing with a large number of items).\n",
    "\n",
    "We introduce this method as a prototypical way to qualitatively evaluate the models we trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code allows us to train a T-SNE based on the \n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "num_items = 25\n",
    "\n",
    "vocab = list(model.wv.vocab)[1:num_items]\n",
    "X = model[vocab]\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(22.5, 18.5)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Qualitative Analysis\n",
    "\n",
    "Experiment with T-SNE by changing the number of displayed items and the rendering with matplotlib.\n",
    "\n",
    "* Visualize the results for models trained on Swiss text and on the review data.\n",
    "    - this should give you an idea how qualitative visualisations can immediately show you potential problems in your model.\n",
    "* Change the displayed item by selecting different parts of the vocabulary\n",
    "    - print(list(model.wv.vocab)) lets you take a look at the vocabulary\n",
    "    - it usally makes sense to apply qualitative analysis to the extreme values represented by a model\n",
    "* Change the way the plot is displayed by changing the size of the figure or the font size until you are satisfied with the results. Take a look at the matplotlib documentation if you want to take it a bit further. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow Iris\n",
    "\n",
    "A typical machine learning workflow:\n",
    "\n",
    "1. Dataset Curation\n",
    "2. Dataset Pre-processing\n",
    "3. Dataset Provision\n",
    "4. Training Configuration\n",
    "5. Model Training Run\n",
    "6. Evaluation\n",
    "7. Iterative Optimisation\n",
    "\n",
    "We will look at the above workflow based on a commonly used machine learning dataset called \"Iris Data Set\".\n",
    "\n",
    "The \"Iris Data Set\" is small dataset with the following stats:\n",
    "\n",
    "* 4 `Features` (Attributes)\n",
    "* 150 `Samples` (Instances, Rows)\n",
    "\n",
    "You can read more a bit more on the dataset here: https://archive.ics.uci.edu/ml/datasets/iris/.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 2] Dataset Pre-Processing\n",
    "\n",
    "The easiest way to load the IRIS dataset is to use the built-in functionality of sci-kit learn.\n",
    "\n",
    "You can load the IRIS dataset with the following commmands:\n",
    "\n",
    "``from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "``\n",
    "\n",
    "To familiarize ourselves with the structure of a dataset and to get to know the tooling we need to load our own datasets we will load the dataset ourselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data\n",
    "\n",
    "The data for the Iris dataset can be downloaded from.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\n",
    "    \n",
    "The web directory contains two files that are important for us:\n",
    "\n",
    "* iris.data\n",
    "* iris.names\n",
    "\n",
    "On a Linux or Mac OS machine you can use the following commands to download the files to a local directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget is a handy command line utility that allows downloading the specified URL\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the  Iris Dataset Format\n",
    "\n",
    "Using the command line (or a text editor) we can inspect that dataset.\n",
    "\n",
    "The `!` operator will allow you to execute command line commands from a Jupyter cell. \n",
    "This should work on all supported operating systems (Mac OS, Linux, Windows).\n",
    "\n",
    "On a mac or linux machine you can make use of the following command line commands:\n",
    "\n",
    "* `head` : Show top n lines of a text file\n",
    "* `tail` : Show last n lines of a text file\n",
    "* `cat`  : Print full content of a text file\n",
    "* `wc -l`: Count number of lines of a text file\n",
    "\n",
    "On a Windows machine the following should work:\n",
    "\n",
    "* `more` : Show content of a text file (might hang in Jupyter)\n",
    "* `type` : Print content of a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\r\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\r\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\r\n",
      "4.6,3.1,1.5,0.2,Iris-setosa\r\n",
      "5.0,3.6,1.4,0.2,Iris-setosa\r\n"
     ]
    }
   ],
   "source": [
    "# head and tail are other useful command line utilities on a linux machine that allow us to see the first n or last \n",
    "# n lines of a text file.\n",
    "\n",
    "# Take your time to inspect both files with the head and tail commands. If you know that a file is not too long you can\n",
    "# also make use of the cat command that prints an entire files contents. For large files this is not advised as it can \n",
    "# easily overpower the javascript based rendering on the browser.\n",
    "\n",
    "!head -n 5 iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the Iris dataset is a `CSV` (Comma Separated Values) file.\n",
    "\n",
    "Each row contains 5 values: 4 `double` values and a `string`.\n",
    "The 4 `double` values represent the 4 `features` (in this case measurements of the plant).\n",
    "The `string` value represents the `class` (type) of plant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 3] Provision: Loading the Dataset into a Dataframe\n",
    "\n",
    "Provisioning data means making data available in the expected format of a library or application program.\n",
    "In our case we will be using the sci-kit learn library and the machine learning algorithms that are available as part of this library. \n",
    "\n",
    "### NumPy Arrays\n",
    "\n",
    "Most algorithms that form part of sci-kit learn accept datasets in form of `NumPy` `arrays` as input.\n",
    "`NumPy` is a library that offers implementations of numerical `arrays`, `multi-dimensional arrays`, `matrices` and mathematical operations that can be applied to these data structures. It is the basis for many machine learning libraries in Python due to the reliability (as in reliable results) and efficiency (as in computational efficiency) of its implementation (in Python and in C). \n",
    "\n",
    "The following links provide some nice introductions.\n",
    "\n",
    "* http://scipy-lectures.org/intro/numpy/array_object.html#what-are-numpy-and-numpy-arrays\n",
    "\n",
    "Frequent commands that we will use initially are:\n",
    "\n",
    "* `import numpy as np` : Importing as `np` is a common convention\n",
    "* `array_named_a.ndim` : Give back the dimensionality of the array (i.e. 1-dimension, 2-dimensional) ...\n",
    "* `array_named_a.shape`: What is the `shape` of the dimensions of the array (one can think of it as number of rows and columns)\n",
    "* `len(array_named_a)` : What is the length of the array (i.e. the number of rows).\n",
    "\n",
    "\n",
    "### Pandas\n",
    "\n",
    "Pandas is a library for handling of dataframes that wraps numpy arrays and tries to simplify their handling and the loading from and to other dataformats.\n",
    "\n",
    "We will start of with the following initial command:\n",
    "\n",
    "* `import pandas as pd` : Importing as `pd` is a common convention\n",
    "* `pd.read_csv()` : Read CSV input into a dataframe\n",
    "\n",
    "In order to understand how to use the read_csv() method we can use the `?` operator as shown in the cell below.\n",
    "\n",
    "This will pull up documentation for the method parameters, the return type, and example usage (the latter at the end of the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Load the Iris.data CSV file with pandas\n",
    "\n",
    "Take a look at the documentation of the read_csv() method and load the `Iris.data` file with pandas.\n",
    "In case you still miss the pandas package use the `!conda install -y` functionality to pull and install the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Iris.data CSV file into a dataframe called iris_dataframe\n",
    "\n",
    "iris_dataframe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3            4\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use the head() method in order to inspect the loaded dataframe.\n",
    "# Your result should look exactly like shown below.\n",
    "# If your result looks different then please have a look at the documentation of the parameters of the read_csv method \n",
    "# and load the data again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `?` operator can also be directly applied to objects. E.g to identify what is iris_dataframe we can apply it as\n",
    "# shown below.\n",
    "\n",
    "iris_dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***: Inspect the iris_dataframe with the shape, ndim and len() attributes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the iris_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create an Input and Response Dataframe\n",
    "\n",
    "If we want to train a supervised machine learning model based on the Iris dataset we have to split our original iris_dataframe into an Input and a Response dataframe.\n",
    "\n",
    "* The `Input` dataframe contains the `features` that are the input for the learning and decision making of the machine learning model.\n",
    "* The `Response` (a.k.a. `Target`) dataframe contains the correct expected values (a.k.a answers) that the system is suppposed to learn.\n",
    "\n",
    "***Iris Setosa Classifier***\n",
    "\n",
    "If we want to train a classifier (a machine learning model that predicts the class/type of Setosa flower based on the 4 measurements) then the content of the `Input` and `Response` dataframes would consist of the following:\n",
    "\n",
    "* `Input`: Each row of the dataframe consists of the 4 measurement values\n",
    "* `Response` : Each row of the dataframe consists of the `class` of the flower \n",
    "\n",
    "To create these two dataframes we can use the functionality provided by pandas:\n",
    "\n",
    "* https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#basics holds the full documentation but is not very easy to read\n",
    "\n",
    "The main functionality we will make use of are:\n",
    "\n",
    "***Selecting columns by label:***\n",
    "\n",
    "We can use `[]` brackets to select a subset of a dataframe. To select a set of columns we can use an array of labels (column headings).\n",
    "\n",
    "E.g. `sample_dataframe[[0,5]]` would give us back a dataframe consisting of the columns with the ***names*** `0` and `5`.\n",
    "\n",
    "Use `sample_dataframe.columns` to get the available columns and their labels listed. \n",
    "\n",
    "\n",
    "***Slicing subsets by row:***\n",
    "\n",
    "This can be achieved by using the `[start:end]` operator.\n",
    "It is a pretty simple way to select rows.\n",
    "\n",
    "* `sample_dataframe[0:5]` : Select row 0,1,2,3,4 of sample_dataframe (***end*** is not inclusive)\n",
    "* `sample_dataframe[start:]` : Select everything from start until the end of the dataframe\n",
    "* `sample_dataframe[:end]` : Select everything from the start until end - 1 index of the dataframe\n",
    "* `sample_dataframe[:]` This would select the full dataframe (all rows)\n",
    "\n",
    "This notation is not limited to dataframes. It works with all lists in Python.  For a list the use of this operator will result in a new copy of the dataframe. For numpy arrays it will result in a view that shares the memory with the source dataframe. \n",
    "\n",
    "\n",
    "The best documentation of the slice operator I know of is provided here: https://stackoverflow.com/questions/509211/understanding-slice-notation?page=1&tab=votes#tab-top\n",
    "\n",
    "Based on the above documentation create the `Input` and `Response` dataframes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are different naming conventions people use for these dataframes\n",
    "# input, response or X,Y are common. input, target is also commonly used\n",
    "# For this notebook we will stick with input and response as shown below\n",
    "iris_dataframe_input = \n",
    "iris_dataframe_response = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise: Inspect the newly created dataframes***\n",
    "\n",
    "Use the tooling we introduced before in order to inspect your newly created dataframes. \n",
    "\n",
    "* `head()`\n",
    "* `shape`\n",
    "* `dim`\n",
    "* `len()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect your dataframes with the above tools in order to get familiar with them\n",
    "# Inspecting the intermediary artifacts in the machine learning workflow is a common and crucial task.\n",
    "# It is easy to imagine how one can be off when sub-setting or slicing through the input data by making a mistake. \n",
    "# These kind of errors are usually disastrous in terms of the outcome of the trained model. The earlier we catch them\n",
    "# the less expensive they are to fix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 4] Training Configuration\n",
    "\n",
    "The next step consist of creating the configuration for the training. \n",
    "\n",
    "The main dependencies for choosing a training set up are:\n",
    "\n",
    "* The data used for training (data type, quality, amount)\n",
    "* The task we want to solve (what we want the machine learning system to achieve)\n",
    "\n",
    "Based on these two aspects designing the training set up consists of the following steps:\n",
    "\n",
    "1. Choose training algorithm\n",
    "2. Create initial configuration for training algorithm\n",
    "\n",
    "## Criteria for Choosing A ML Algorithm\n",
    "\n",
    "Some main criteria for choosing a training algorithm are the following:\n",
    "\n",
    "* Task Fit : I.e. can the task we want to solve with ML be solved with the given algorithm\n",
    "* Scalability: How scalable in terms of the shape (columns, rows) of the input data is the algorithm \n",
    "    * The amount of features has a major impact on the scalability of algorithms\n",
    "    * The amount of samples (rows) has a major impact on the execution time of the algorithm\n",
    "* Expected Performance: What is the expected accuracy of the algorithm.\n",
    "* Interpretability: How easy, hard is it to understand what is happening in the algorithm. How hard would it be to 'debug' the behaviour of the algorithm.\n",
    "* Updatable Learning: Can the learned model be updated with more data at a later stage.\n",
    "* Availability: In the pragmatic sense; is a trusty implementation of the algorithm available (also from a license perspective).\n",
    "* Solution requirements: Do we have requirements from the software solution side. Maximum latency, memory limitations, etc ... . \n",
    "\n",
    "As the above list highlights, choosing the 'right' algorithm is a complex tasks with many potential considerations.\n",
    "On the flip side it means that making the right choices has massive potential value. \n",
    "\n",
    "## Choosing an Initial Configuration\n",
    "\n",
    "The choice of an initial training configuration often depends mainly on:\n",
    "\n",
    "* Stats of the training data \n",
    "     * Hyperparameters often allow us to adjust the training to the amount of the training data\n",
    "* Experience or documented well working configurations \n",
    "     * This is often based on identifying `baselines` that worked well on data that we deem similar to our training data.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Task\n",
    "\n",
    "One machine learning task that fits well to the Iris dataset is `classification`.\n",
    "`Classification` is the task of assigning a `class` (type) to samples based on the input features. \n",
    "For the Iris dataset that translates to using the features (4 measurements) as input for taking the decision (to classify) which type of Iris plant it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Algorithm\n",
    "\n",
    "Lets say we aim for classifing the type of Iris based on the input.\n",
    "\n",
    "This is the `task` that we want to solve.\n",
    "\n",
    "The choice of `task` informs the choice of our machine learning algorithm.\n",
    "In this case we choose a `classification algorithm`, also called `classifier`.\n",
    "\n",
    "A commonly used classification algorithm is called `logistic regression`.\n",
    "For our initial classification experiments we will make use of this algorithm type.\n",
    "\n",
    "We can make set up this algorithm by the following steps:\n",
    "\n",
    "* Import: `from sklearn.linear_model import LogisticRegression`\n",
    "* Instantiate Model Class: `LogisticRegression`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Set up the logistic regression model\n",
    "\n",
    "\n",
    "\n",
    "classifier = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 5] Model Training Run\n",
    "\n",
    "Algorithms in scikit-learn can be trained by using the `fit` method. Calling it `fit` is based on the process of `fitting` the model's weights (also called model parameters) during training.\n",
    "`Fitting` means that the weights of the model are adjusted during the training (a.k.a learning) phase based on the input data we have seen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run\n",
    "\n",
    "Training a model based on the input data is often referred to as executing or making a `training run`; or simply just a `run`. \n",
    "\n",
    "Common ways to use this terminology are e.g.:\n",
    "\n",
    "* \"I have made a run with the following input data and these hyperparameters\".\n",
    "* Which parameters (meaning hyperparameters) were used for the run?\n",
    "* What was the best run?\n",
    "\n",
    "Running a training in scikit-learn can be based on executing the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = classifier.fit(iris_dataframe_input, iris_dataframe_response.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Instantiating with Parameters\n",
    "\n",
    "WHen we first ran the classifier we got multiple warnings.\n",
    "Take a look a the documentation of the classifier object with the `?` operator.\n",
    "Then instantiate the classifier with the correct parameters to avoid the warnings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our trained model\n",
    "\n",
    "For a first easy test we can used the `predict` method as shown below.\n",
    "The predict method allows us to pass in data with the same format as the input data.\n",
    "\n",
    "As an exercise try passing in data with a different format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "trained_model.predict([[0.5,5.0,2.0,1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the slicing approach we introduced earlier to pass in a subset of the rows of our input data to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.predict(iris_dataframe_input[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 6] Evaluation\n",
    "\n",
    "All machine learning models provide a default metric that can be accessed via the score method. \n",
    "\n",
    "In the case of logistic regression it will provide us mean accuracy on the given test data and label.\n",
    "\n",
    "(see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = trained_model.score(iris_dataframe_input, iris_dataframe_response.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train and Test\n",
    "\n",
    "\n",
    "If we use the same samples for the training of the model and its evaluation, the measured performance is likely a bad\n",
    "indicator of the performance we can expect when we let the model take decisions on data it has not seen before.\n",
    "\n",
    "Testing with the same data that was used for training will usually result in a much higher score. This is due to the effect of fitting the weights the model learns completely on the seen samples.\n",
    "\n",
    "It is therefore custom to split the available labeled data into two parts before we start training of the ML model:\n",
    "* Train Portion\n",
    "* Test Portion\n",
    "\n",
    "The train portion is the part of the dataset that is used for the training of the model.\n",
    "The test portion is the part of the dataset that is ***excluded*** from the training of the model.\n",
    "This is also called ***holding out*** part of the data. This ***unseen data*** (i.e. data that the model has never seen before) is then used to evaluate how well the model is doing.\n",
    "\n",
    "Typical splits between train and test are 70/30 or 80/20. \n",
    "\n",
    "\n",
    "### Considering the Order of the Input Data\n",
    "\n",
    "When splitting the data into a test and train portion it is important to try to split the data in random fashion. \n",
    "We try to avoid an in-balance for the frequency of classes in either test or train, or an in-balance in terms of 'hard' and 'easy' cases when splitting the data.\n",
    "\n",
    "The best way to achieve this, is to randomly sample (or shuffle) the input data before making the split. \n",
    "\n",
    "Pandas offers us the `sample(frac=double_value)` method as a simple way to do this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create Train and Test datasets\n",
    "\n",
    "* Create a Train and Test portion of the input data.\n",
    "* You should take the original `iris_dataframe` as input for this\n",
    "* The iris_dataframe is pre-sorted by class. It is a good example of a case where shuffling or sampling from data before the split is a must. So you should start with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable for the shuffled dataframe\n",
    "iris_df_shuffled =\n",
    "\n",
    "# The test and train dataframes containing all features\n",
    "iris_df_train =\n",
    "iris_df_test =\n",
    "\n",
    "# Split the dataframes into input and response\n",
    "iris_df_train_input = \n",
    "iris_df_train_response = \n",
    "\n",
    "iris_df_test_input =\n",
    "iris_df_test_response = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Train Classifier on Train Dataset\n",
    "\n",
    "Take the train dataset you have created and use it to re-train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Evaluate Classifier with Test Dataset\n",
    "\n",
    "* Use the test dataset you have created and evaluate the classifier trained on the train dataset.\n",
    "* Execute the splitting, training and evaluation multiple times and observe what happens to the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

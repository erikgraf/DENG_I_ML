{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow Digits\n",
    "\n",
    "Based on working our way through the Machine Learning Workflow Iris notebook this notebook focuses on\n",
    "providing some exercise for establishing a machine learning workflow.\n",
    "\n",
    "1. Dataset Curation\n",
    "2. Dataset Pre-processing\n",
    "3. Dataset Provision\n",
    "4. Training Configuration\n",
    "5. Model Training Run\n",
    "6. Evaluation\n",
    "7. Iterative Optimisation\n",
    "\n",
    "You can read more a bit more on the dataset here: http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 2] Dataset Pre-Processing\n",
    "\n",
    "The easiest way to load the Digits dataset is to use the built-in functionality of sci-kit learn.\n",
    "\n",
    "You can load the IRIS dataset with the following commmands:\n",
    "\n",
    "``from sklearn import datasets\n",
    "iris = datasets.load_digits()\n",
    "``\n",
    "\n",
    "However, to practice the initial steps of the machine learning workflow we will \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Downloading the Data\n",
    "\n",
    "The data for the Digits dataset can be downloaded from.\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/\n",
    " \n",
    "\n",
    "On a Linux or Mac OS machine you can use the following commands to download the files to a local directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget is a handy command line utility that allows downloading the specified URL\n",
    "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Inspecting the  Digits Dataset Format\n",
    "\n",
    "Using the command line (or a text editor) we can inspect that dataset.\n",
    "\n",
    "The `!` operator will allow you to execute command line commands from a Jupyter cell. \n",
    "This should work on all supported operating systems (Mac OS, Linux, Windows).\n",
    "\n",
    "On a mac or linux machine you can make use of the following command line commands:\n",
    "\n",
    "* `head` : Show top n lines of a text file\n",
    "* `tail` : Show last n lines of a text file\n",
    "* `cat`  : Print full content of a text file\n",
    "* `wc -l`: Count number of lines of a text file\n",
    "\n",
    "On a Windows machine the following should work:\n",
    "\n",
    "* `more` : Show content of a text file (might hang in Jupyter)\n",
    "* `type` : Print content of a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head and tail are other useful command line utilities on a linux machine that allow us to see the first n or last \n",
    "# n lines of a text file.\n",
    "\n",
    "# Take your time to inspect both files with the head and tail commands. If you know that a file is not too long you can\n",
    "# also make use of the cat command that prints an entire files contents. For large files this is not advised as it can \n",
    "# easily overpower the javascript based rendering on the browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 3] Provision: Loading the Dataset into a Dataframe|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Load the Digits CSV files with pandas\n",
    "\n",
    "Lets create the test and train portions from the beginning based on the structure of the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the head() method in order to inspect the loaded dataframe.\n",
    "# Your result should look exactly like shown below.\n",
    "# If your result looks different then please have a look at the documentation of the parameters of the read_csv method \n",
    "# and load the data again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***: Inspect the dataframes with the shape, ndim and len() attributes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the digits dataframes you have created for test and train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise: Create the Input and Response Dataframes for Test and Train Portions***\n",
    "\n",
    "Use the tooling we introduced before in order to inspect your newly created dataframes. \n",
    "\n",
    "* `head()`\n",
    "* `shape`\n",
    "* `dim`\n",
    "* `len()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect your dataframes with the above tools in order to get familiar with them\n",
    "# Inspecting the intermediary artifacts in the machine learning workflow is a common and crucial task.\n",
    "# It is easy to imagine how one can be off when sub-setting or slicing through the input data by making a mistake. \n",
    "# These kind of errors are usually disastrous in terms of the outcome of the trained model. The earlier we catch them\n",
    "# the less expensive they are to fix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 4] Training Configuration\n",
    "\n",
    "The next step consist of creating the configuration for the training. \n",
    "\n",
    "The main dependencies for choosing a training set up are:\n",
    "\n",
    "* The data used for training (data type, quality, amount)\n",
    "* The task we want to solve (what we want the machine learning system to achieve)\n",
    "\n",
    "Based on these two aspects designing the training set up consists of the following steps:\n",
    "\n",
    "1. Choose training algorithm\n",
    "2. Create initial configuration for training algorithm\n",
    "\n",
    "## Criteria for Choosing A ML Algorithm\n",
    "\n",
    "Some main criteria for choosing a training algorithm are the following:\n",
    "\n",
    "* Task Fit : I.e. can the task we want to solve with ML be solved with the given algorithm\n",
    "* Scalability: How scalable in terms of the shape (columns, rows) of the input data is the algorithm \n",
    "    * The amount of features has a major impact on the scalability of algorithms\n",
    "    * The amount of samples (rows) has a major impact on the execution time of the algorithm\n",
    "* Expected Performance: What is the expected accuracy of the algorithm.\n",
    "* Interpretability: How easy, hard is it to understand what is happening in the algorithm. How hard would it be to 'debug' the behaviour of the algorithm.\n",
    "* Updatable Learning: Can the learned model be updated with more data at a later stage.\n",
    "* Availability: In the pragmatic sense; is a trusty implementation of the algorithm available (also from a license perspective).\n",
    "* Solution requirements: Do we have requirements from the software solution side. Maximum latency, memory limitations, etc ... . \n",
    "\n",
    "As the above list highlights, choosing the 'right' algorithm is a complex tasks with many potential considerations.\n",
    "On the flip side it means that making the right choices has massive potential value. \n",
    "\n",
    "## Choosing an Initial Configuration\n",
    "\n",
    "The choice of an initial training configuration often depends mainly on:\n",
    "\n",
    "* Stats of the training data \n",
    "     * Hyperparameters often allow us to adjust the training to the amount of the training data\n",
    "* Experience or documented well working configurations \n",
    "     * This is often based on identifying `baselines` that worked well on data that we deem similar to our training data.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Task\n",
    "\n",
    "By now it should be pretty clear what task fits well with the detection of hand-written digit images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Choose Classification Algorithm\n",
    "\n",
    "For this ML task we can consider a variety of classifiers available in sci-kit learn:\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "The above listing represent some very widely used main classes of classifiers.\n",
    "To proceed choose any of the above classifiers and instantiate it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Set up the logistic regression model\n",
    "\n",
    "\n",
    "\n",
    "classifier = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Step 5] Model Training Run - Establishing a Baseline\n",
    "\n",
    "Algorithms in scikit-learn can be trained by using the `fit` method. Calling it `fit` is based on the process of `fitting` the model's weights (also called model parameters) during training.\n",
    "`Fitting` means that the weights of the model are adjusted during the training (a.k.a learning) phase based on the input data we have seen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Establishing a Baseline\n",
    "\n",
    "Based on the choosen classifier and the test and train data, lets establish a first baseline for the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your trained model\n",
    "\n",
    "As before, use the `predict()` method of the trained model and test the classification by passing in new arrays or slicing part of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Step 6] Evaluation\n",
    "\n",
    "All machine learning models provide a default metric that can be accessed via the score method. \n",
    "\n",
    "Record your first baseline by using the score method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Confusion Matrix\n",
    "\n",
    "The following code allows us to instantiate and plot a confusion matrix.\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns; sns.set()\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    mat = confusion_matrix(array_expected_responses, trained_model.predict(test_input))\n",
    "\n",
    "    sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "    plt.xlabel('predicted value')\n",
    "    plt.ylabel('true value');\n",
    "\n",
    "\n",
    "\n",
    "A confusion matrix is a very handy tool to identify where the classifier is likely to make errors.\n",
    "\n",
    "The True Positive (correct decisions) are always depicted on the diagonal of the confusion matrix.\n",
    "If the axis are labeled like in the example configuration below it is easy to identify the errors the model makes.\n",
    "\n",
    "Adapt the above code to create a confusion matrix for your classifier. \n",
    "Evaluate the result of different classifiers.\n",
    "\n",
    "* Do different classifiers make different kinds of mistakes?\n",
    "* Can we identify any patterns in the behaviour of classifiers?\n",
    "* Using the oonfusion matrix to identify the different classifiers should give you a feeling for its usefulness. A score is only a number and does not provide much else. The confusion matrix allows us to develop more insights with regard to the actual behaviour of a trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Cross-Validation\n",
    "\n",
    "As we learnt before it is common practice to operate based on a split of test and train data. \n",
    "\n",
    "Testing with unseen data is more realistic than executing the test with data that has already been processed by the trained model. \n",
    "\n",
    "However, we have also seen that the actual split (i.e. which samples end up in train and which in test) can have quite a significant impact on the observed performance.\n",
    "\n",
    "In order to mitigate this effect the method of cross-validation has been invented.\n",
    "Cross-validation simply means that I attempt to split my data `n` times into test and train portions and then run and evaluate my model `n` times with these different splits.\n",
    "\n",
    "One generally speaks of n-fold cross-validation. That means, if we speak of 10-fold cross-validation we will split the data randomly 10 times into a test and train portion, run the training 10 times on these splits, and evaluate 10 times.\n",
    "\n",
    "The main goal is to evaluate outliers in terms of exceptionally good or bad performance that are mainly caused by generating a 'lucky' or 'bad' split of the data.\n",
    "\n",
    "Even though you are using cross-validation we should hold out a dedicated test set.\n",
    "\n",
    "The following link provides instructions of how to apply cross-validation:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics\n",
    "\n",
    "Please configure cross-validation and run evaluations on a couple of the classifiers with it.\n",
    "\n",
    "What are some limiting factors for applying cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
